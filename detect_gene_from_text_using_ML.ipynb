{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b1da43-ed69-4322-9eb8-4449d5405732",
   "metadata": {},
   "source": [
    "# Classify a text document as True/False based on whether it contains gene sequence or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c7e4a-abc3-42df-8ecd-25efae30af53",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae0f78a-c45b-497a-9ecc-9b9fe322c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, Tuple, Optional, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e3ab2d-6ce4-439e-b21e-aa9147c08d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aba70bb-b358-45bc-9549-3ee09b3e602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a0b46c-71b7-4e4d-8197-cc93e52760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223f5f00-f0a6-4d58-b91b-4315d14d9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa07f32-b735-48d4-afdc-758078fc99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c932766c-eabd-489f-a57a-c17046d0105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f167f6ca-e8bc-47e0-99f0-e5b3f67d9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4503b798-3cb2-44b9-b25c-3023fd5a2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27f1dde-6548-4bba-b42b-326a43e1335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb796368-c972-44ce-98cf-a591cc3b2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd96ba-f09e-4ccc-ac2c-6170227c8698",
   "metadata": {},
   "source": [
    "## Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79acc8a-3fdd-4d8a-9075-1bf86aee499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df):\n",
    "    # Compute split index - train and test\n",
    "    split_idx = int(len(df) * 0.85)  # first 85%\n",
    "    \n",
    "    # First 85% and remaining 15%\n",
    "    train = df[:split_idx]\n",
    "    test = df[split_idx:]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "277090ee-c85b-40b1-ad81-60fc67be1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lists(list_of_lists):\n",
    "    out = []\n",
    "    for lst in list_of_lists:\n",
    "        lst_copy = lst[:]            # avoid modifying original\n",
    "        random.shuffle(lst_copy)     # shuffle in place\n",
    "        out.append(lst_copy)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f23b499-bb4c-4fcb-b642-301b5557a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random strings of text\n",
    "\n",
    "CHARS = string.ascii_letters + string.digits + \" .,!?$%&*#@/\\n\"\n",
    "\n",
    "def random_garbage(n):\n",
    "    return ''.join(random.choices(CHARS, k=n))\n",
    "\n",
    "def generate_fake_content():\n",
    "    prefix_len = random.randint(0, 5000)\n",
    "    suffix_len = random.randint(0, 5000)\n",
    "\n",
    "    prefix = random_garbage(prefix_len)\n",
    "    suffix = random_garbage(suffix_len)\n",
    "\n",
    "    return f\"{prefix}{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61abad6e-6eae-4c7d-8c50-43e79ca9c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of common bioinformatics/genomic file extensions.\n",
    "# To identify files matching these extensions are classified as 'genomic data'.\n",
    "GENOMIC_EXTENSION = [\n",
    "    '.fastq', '.fq', '.fasta', '.fa', '.fna', '.gb', '.gff', '.gff3', '.gtf',\n",
    "    '.sam', '.bam', '.cram', '.vcf', '.bcf', '.wig', '.bed', '.bigwig', '.tbi',\n",
    "    '.tabix', '.h5', '.hdf5', # HDF5 often used for single-cell data (e.g., Anndata)\n",
    "    ]\n",
    "\n",
    "# A set of common document extensions for text extraction.\n",
    "DOCUMENT_EXTENSIONS = ['.txt', '.pdf', '.doc', '.docx']\n",
    "\n",
    "def analyze_file(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a file path to determine if it is genomic data or a document,\n",
    "    and returns the classification or the document content.\n",
    "\n",
    "    Args:\n",
    "        file_path: The full path to the file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (is_genomic_extension, file_extension, content_or_none_or_emptymessage).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return (False, None, None)\n",
    "\n",
    "    # Check for empty file before reading\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        return (False, None, \"EMPTY FILE\")\n",
    "\n",
    "    # Get the file name and extension\n",
    "    file_name = os.path.basename(file_path)\n",
    "    _, ext = os.path.splitext(file_name)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    # 1. Check for Genomic Data\n",
    "    if ext in GENOMIC_EXTENSIONS:\n",
    "        return (True, ext, None)\n",
    "\n",
    "    # 2. Check for Document Data and Extract Content\n",
    "    elif ext in DOCUMENT_EXTENSIONS:\n",
    "        if ext == '.txt':\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "                return (False, ext, content.strip())\n",
    "            except Exception as e:\n",
    "                return (False, ext, f\"Could not read text file: {e}\")\n",
    "\n",
    "        elif ext == '.pdf':\n",
    "            reader = PdfReader(file_path)\n",
    "            content = \"\".join([page.extract_text() for page in reader.pages])\n",
    "            if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "            return (False, ext, content)\n",
    "\n",
    "        elif ext in ['.doc', '.docx']:\n",
    "            document = Document(file_path)\n",
    "            content = \"\\n\".join([paragraph.text for paragraph in document.paragraphs])\n",
    "            if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "            return (False, ext, content)\n",
    "\n",
    "    # 3. Handle Unknown/Other Files\n",
    "    return (False, ext, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d670ab71-c61c-47a5-a29c-f4e47810a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_genomic, file_type, content = analyze_file(\"xyz.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9873f-95e6-4562-8994-dad19e1a781b",
   "metadata": {},
   "source": [
    "## Load data, prepare train and test data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "728cf29c-b494-43ad-bb44-a5337fff0312",
   "metadata": {},
   "source": [
    "### Curating our dataset -> Methodology\n",
    "\n",
    "To train our ML model:\n",
    "\n",
    "english and seq -> pos(actual), pos(actual+english), neg(english)\n",
    "                   200          200                  200          -> 170 train, 30 test\n",
    "\n",
    "text and seq -> pos(actual), pos(actual+text) \n",
    "                1000         1000               -> 850 train, 150 test\n",
    "\n",
    "text_and_seq_alternate_random_characters -> pos(actual), pos(actual with alt rand char+english), neg(english)\n",
    "                                            200          200                                     200            -> 170 train, 30 test\n",
    "\n",
    "text and seq compressed -> pos(actual), pos(actual compressed), pos(actual compressed + english), neg(english)\n",
    "                           200          200                     200                               200            -> 170 train, 30 test\n",
    "\n",
    "text and seq compressed and replaced -> pos (actual), pos (actual compressed), pos (compressed_replaced + english), neg (english)\n",
    "                                        200           200                      200                                  200              -> 170 train, 30 test\n",
    "\n",
    "text and seq replaced with other characters -> pos(actual), pos(replaced + english), neg(english)\n",
    "                                            200             200                      200            -> 170 train, 30 test\n",
    "\n",
    "single (fake) dna -> neg\n",
    "                    100  -> 85 train, 15 test\n",
    "\n",
    "randomly generated text -> neg\n",
    "                        3300  (calculated based on above imbalance of pos and neg) -> 2805 train, 495 test\n",
    "\n",
    "\n",
    "Total input data samples = 8800\n",
    "Total pos = 4400 (50%)\n",
    "Total neg = 4400 (50%)\n",
    "Train = 85%, Test = 15%\n",
    "\n",
    "\n",
    "Distributions are same across train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cec31fe-6b0e-4799-88f6-c91fdb4c1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train=[]\n",
    "pos_test=[]\n",
    "neg_train=[]\n",
    "neg_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb451c50-9b97-456a-976f-1ded2dd5cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"dataset_dna/english_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test, neg_train, neg_test = shuffle_lists([pos_train, pos_test, neg_train, neg_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2fe41251-c750-4b68-acb6-240d13994da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 200)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train)+len(pos_test), len(neg_train)+len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "befda429-92ce-4a1f-b5f8-c551e622a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "\n",
    "path = \"dataset_dna/text_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['generated'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test = shuffle_lists([pos_train, pos_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4834dd11-053d-4343-9419-e6aca06fa63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 200)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train)+len(pos_test), len(neg_train)+len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55f10519-578b-414f-b779-7ccb4ade3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"dataset_dna/text_and_seq_alternate_random_characters\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test, neg_train, neg_test = shuffle_lists([pos_train, pos_test, neg_train, neg_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abb80e68-da1f-4d77-aa2e-38ca15a468a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "p3=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"dataset_dna/text_and_seq_compressed\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['compressed'].values[0])\n",
    "    p3.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "p3_train, p3_test = split_train_test(p3)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train + p3_train\n",
    "pos_test = pos_test + p1_test + p2_test + p3_test\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test, neg_train, neg_test = shuffle_lists([pos_train, pos_test, neg_train, neg_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcb80c18-261f-4d2f-a3b0-52d8d167b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "p3=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"dataset_dna/text_and_seq_compressed_and_replaced\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['compressed'].values[0])\n",
    "    p3.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "p3_train, p3_test = split_train_test(p3)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train + p3_train\n",
    "pos_test = pos_test + p1_test + p2_test + p3_test\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test, neg_train, neg_test = shuffle_lists([pos_train, pos_test, neg_train, neg_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dc62e55-9781-4d8d-9ac1-92bea9c248cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"dataset_dna/text_and_seq_replaced_with_other_characters\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "pos_train, pos_test, neg_train, neg_test = shuffle_lists([pos_train, pos_test, neg_train, neg_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a9aad32-cca5-4e90-af00-3ec42d5e6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = []\n",
    "for file in os.listdir('dataset_dna/single_dna'):\n",
    "    try:\n",
    "        with open('dataset_dna/single_dna/'+file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "    except:\n",
    "        continue\n",
    "    n1.append(text)\n",
    "n1 = [s for s in n1 if len(s) <= 1000]\n",
    "n1 = random.sample(n1, 100)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "neg_train, neg_test = shuffle_lists([neg_train, neg_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb39915d-525a-4495-92d9-bca06490fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [] \n",
    "\n",
    "for i in range(3300):\n",
    "    n1.append(generate_fake_content())\n",
    "\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "# shuffle global list after adding\n",
    "neg_train, neg_test = shuffle_lists([neg_train, neg_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4310572-a004-4018-bbcb-ce2343679682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15*200 -> divide english dataset by 30(test) and 170(train)\n",
    "0.15*1000 -> divide seq dataset by 150(test) and 850(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f1f0f3f-e220-46b4-a3ad-34bc762998c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "neg=[]\n",
    "\n",
    "# first 100 in pos and neg dataset\n",
    "path = \"dataset_dna/english_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    df = pd.read_csv(file_path)\n",
    "    pos.append(df['text_with_dna'].values[0])\n",
    "    neg.append(df['text_without_dna'].values[0])\n",
    "\n",
    "# Compute split index - train and test\n",
    "split_idx = int(len(pos) * 0.85)  # first 85%\n",
    "\n",
    "# First 85% and remaining 15%\n",
    "pos_train = pos[:split_idx]\n",
    "pos_test = pos[split_idx:]\n",
    "neg_train = neg[:split_idx]\n",
    "neg_test = neg[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb6da717-31d3-413b-86fa-d4125cf21561",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_actual_dna=[] # 1000 pos\n",
    "positives_dna_with_random_text=[] # 1000 pos\n",
    "path = \"dataset_dna/text_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    df = pd.read_csv(file_path)\n",
    "    positives_actual_dna.append(df['sequence'].values[0])\n",
    "    positives_dna_with_random_text.append(df['generated'].values[0])\n",
    "\n",
    "# Compute split index - train and test\n",
    "split_idx = int(len(positives_actual_dna) * 0.85)  # first 85%\n",
    "\n",
    "# First 85% and remaining 15%\n",
    "positives_actual_dna_train = positives_actual_dna[:split_idx]\n",
    "positives_actual_dna_test = positives_actual_dna[split_idx:]\n",
    "positives_dna_with_random_text_train = positives_dna_with_random_text[:split_idx]\n",
    "positives_dna_with_random_text_test = positives_dna_with_random_text[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27b9c714-6c69-4d65-8c25-4e23f3779d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_fake_dna=[] # 169 negative\n",
    "path = \"dataset_dna/single_dna\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            char_count = len(text)\n",
    "        if(char_count<=2000):\n",
    "            negatives_fake_dna.append(text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Compute split index - train and test\n",
    "split_idx = int(len(negatives_fake_dna) * 0.85)  # first 85%\n",
    "\n",
    "# First 85% and remaining 15%\n",
    "negatives_fake_dna_train = negatives_fake_dna[:split_idx]\n",
    "negatives_fake_dna_test = negatives_fake_dna[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1aefa7a6-1ff2-4daf-b8b7-8068be94cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n",
      "330\n",
      "313\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# train pos\n",
    "print(len(pos_train) + len(positives_actual_dna_train) + len(positives_dna_with_random_text_train))\n",
    "# test pos\n",
    "print(len(pos_test) + len(positives_actual_dna_test) + len(positives_dna_with_random_text_test))\n",
    "# train neg\n",
    "print(len(neg_train) + len(negatives_fake_dna_train) + 1557)\n",
    "# test neg\n",
    "print(len(neg_test) + len(negatives_fake_dna_test) + 274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3c9b84a-a24c-41a3-97e4-2bbb5da6d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557\n",
      "274\n",
      "total neg samples to be generated: 1831\n"
     ]
    }
   ],
   "source": [
    "print(1870 - 313) # random text (neg) train\n",
    "print(330 - 56) # random text (neg) test\n",
    "print('total neg samples to be generated:',(1870 - 313)+(330 - 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40281253-25fd-4ac8-8db6-341d55cb38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_text_neg = [] \n",
    "\n",
    "for i in range(1831):\n",
    "    random_text_neg.append(generate_fake_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d1cb846-ad76-4c04-8dd5-bbfaad5fb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute split index - train and test\n",
    "split_idx = int(len(random_text_neg) * 0.85)  # first 85%\n",
    "\n",
    "# First 85% and remaining 15%\n",
    "random_text_neg_train = random_text_neg[:split_idx]\n",
    "random_text_neg_test = random_text_neg[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "540c2966-1117-4016-8ee5-ddf964e30c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pos\n",
    "train_pos = pos_train+positives_actual_dna_train+positives_dna_with_random_text_train\n",
    "test_pos = pos_test+positives_actual_dna_test+positives_dna_with_random_text_test\n",
    "train_neg = neg_train+negatives_fake_dna_train+random_text_neg_train \n",
    "test_neg=neg_test+negatives_fake_dna_test+random_text_neg_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1bf0173-07c9-4a51-a590-8fac1b8df890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1870, 330, 1869, 331)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos), len(test_pos), len(train_neg), len(test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07201c-cfd7-471e-b6f3-7a74a9cb759e",
   "metadata": {},
   "source": [
    "### Build train and test set from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04118e7d-1d7e-4244-b28b-96f5f6581e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3740, 660, 3740, 660)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train), len(pos_test), len(neg_train), len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "735cdbd2-e6f9-4f54-8b54-cc73a9877b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pos_train\n",
    "test_pos = pos_test\n",
    "train_neg = neg_train\n",
    "test_neg = neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e9b97ae-40b9-4173-8c1e-422f7becc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "\n",
    "random.shuffle(train_pos)\n",
    "random.shuffle(train_neg)\n",
    "# create pos dataframe\n",
    "df_train_pos = pd.DataFrame(train_pos, columns=[\"data\"])\n",
    "df_train_pos[\"label\"] = 1\n",
    "\n",
    "# create neg dataframe\n",
    "df_train_neg = pd.DataFrame(train_neg, columns=[\"data\"])\n",
    "df_train_neg[\"label\"] = 0\n",
    "\n",
    "# combine\n",
    "df_combined = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "\n",
    "#shuffle\n",
    "df_shuffled = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_shuffled.to_pickle('df_train.pkl')\n",
    "\n",
    "X = df_shuffled[\"data\"].values\n",
    "y = df_shuffled[\"label\"].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,\n",
    "                     test_size=0.125,\n",
    "                     random_state=42,\n",
    "                     stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "babb9192-9bc8-47b9-90b0-87d41f7d4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET\n",
    "\n",
    "random.shuffle(test_pos)\n",
    "random.shuffle(test_neg)\n",
    "# create pos dataframe\n",
    "df_test_pos = pd.DataFrame(test_pos, columns=[\"data\"])\n",
    "df_test_pos[\"label\"] = 1\n",
    "\n",
    "# create neg dataframe\n",
    "df_test_neg = pd.DataFrame(test_neg, columns=[\"data\"])\n",
    "df_test_neg[\"label\"] = 0\n",
    "\n",
    "# combine\n",
    "df_combined = pd.concat([df_test_pos, df_test_neg], ignore_index=True)\n",
    "\n",
    "#shuffle\n",
    "df_shuffled = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_shuffled.to_pickle('df_test.pkl')\n",
    "\n",
    "X_test = df_shuffled[\"data\"].values\n",
    "y_test = df_shuffled[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e1a11-ced2-44e0-80ad-1f8252ba9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive: 100(english), 500(actual dna), 600(text+actual dna)\n",
    "negative: 100(english), 1000(text random), 100(fake dna sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "15c6d20a-99cd-4fbb-a294-11f8b5969895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all positive and negative samples\n",
    "\n",
    "pos=[]\n",
    "neg=[]\n",
    "\n",
    "# first 100 in pos and neg dataset\n",
    "path = \"dataset_dna/english_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    df = pd.read_csv(file_path)\n",
    "    pos.append(df['text_with_dna'].values[0])\n",
    "    neg.append(df['text_without_dna'].values[0])\n",
    "\n",
    "positives_actual_dna=[] # 1000 pos\n",
    "positives_dna_with_random_text=[] # 1000 pos\n",
    "path = \"dataset_dna/text_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    df = pd.read_csv(file_path)\n",
    "    positives_actual_dna.append(df['sequence'].values[0])\n",
    "    positives_dna_with_random_text.append(df['generated'].values[0])\n",
    "\n",
    "negatives_fake_dna=[] # 169 negative\n",
    "path = \"dataset_dna/single_dna\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            char_count = len(text)\n",
    "        if(char_count<=2000):\n",
    "            negatives_fake_dna.append(text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9002a88a-849d-4ca0-9298-9d8201d7f3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_text_neg = [] #1000\n",
    "\n",
    "for i in range(1000):\n",
    "    random_text_neg.append(generate_fake_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8051ba61-2ba9-4cc2-8f63-3312c40cc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = random.sample(positives_actual_dna, 500) # 1000 pos, take 500 from here\n",
    "p2 = random.sample(positives_dna_with_random_text, 600) # 1000 pos, take 600 from here\n",
    "\n",
    "n1 = random.sample(random_text_neg, 1000) # take 1000 from here\n",
    "n2 = random.sample(negatives_fake_dna, 100) # take 100 from here\n",
    "\n",
    "pos = pos + p1 + p2\n",
    "neg = neg + n1 + n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bf7df5fa-8db2-4493-89f6-4cc114f4987a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(pos)\n",
    "random.shuffle(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1d2510f6-f5d8-4fb8-9abd-e48322c42b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pos dataframe\n",
    "df_pos = pd.DataFrame(pos, columns=[\"data\"])\n",
    "df_pos[\"label\"] = 1\n",
    "\n",
    "# create neg dataframe\n",
    "df_neg = pd.DataFrame(neg, columns=[\"data\"])\n",
    "df_neg[\"label\"] = 0\n",
    "\n",
    "# combine\n",
    "df_combined = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "\n",
    "#shuffle\n",
    "df_shuffled = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b85bfcc2-3a90-4af9-a01f-77d7e10100ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785 255 360\n"
     ]
    }
   ],
   "source": [
    "# Suppose df has columns: \"data\" and \"label\"\n",
    "X = df_shuffled[\"data\"].values\n",
    "y = df_shuffled[\"label\"].values\n",
    "\n",
    "# First split: Train+Val vs Test (e.g., 20% test)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Second split: Train vs Val (e.g., 10% of original dataset = 12.5% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval,\n",
    "                     test_size=0.125,\n",
    "                     random_state=42,\n",
    "                     stratify=y_trainval)\n",
    "\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f560f-0abc-4acc-a345-5095d9efd283",
   "metadata": {},
   "source": [
    "## Training and Predictions - 1. Preprocessing + ML (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4968fc2-9180-4152-9654-1651696268d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "# Preprocessing\n",
    "###############################################\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Very minimal preprocessing:\n",
    "    - Lowercase\n",
    "    - Remove whitespace newlines\n",
    "    We DO NOT filter characters, because DNA may be obfuscated.\n",
    "    \"\"\"\n",
    "    return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "###############################################\n",
    "# K-mer Extraction\n",
    "###############################################\n",
    "\n",
    "def get_kmers(text, k=3):\n",
    "    \"\"\"Extract k-mers without filtering characters.\"\"\"\n",
    "    kmers = []\n",
    "    for i in range(len(text) - k + 1):\n",
    "        kmers.append(text[i:i+k])\n",
    "    return kmers\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Entropy Calculation\n",
    "###############################################\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy over all characters.\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    from math import log2\n",
    "    freq = {}\n",
    "    for c in text:\n",
    "        freq[c] = freq.get(c, 0) + 1\n",
    "\n",
    "    entropy = 0\n",
    "    for c in freq:\n",
    "        p = freq[c] / len(text)\n",
    "        entropy -= p * log2(p)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Markov Transition Features\n",
    "###############################################\n",
    "\n",
    "def markov_features(text, alphabet):\n",
    "    \"\"\"\n",
    "    Build a Markov transition probability vector from:\n",
    "    P(next_char | current_char)\n",
    "\n",
    "    The output is flattened row-major into a single vector.\n",
    "    \"\"\"\n",
    "    if len(text) < 2:\n",
    "        return np.zeros(len(alphabet) * len(alphabet))\n",
    "\n",
    "    index = {c: i for i, c in enumerate(alphabet)}\n",
    "\n",
    "    # Transition counts\n",
    "    trans = np.zeros((len(alphabet), len(alphabet)))\n",
    "\n",
    "    for a, b in zip(text[:-1], text[1:]):\n",
    "        if a in index and b in index:\n",
    "            trans[index[a], index[b]] += 1\n",
    "\n",
    "    row_sums = trans.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  \n",
    "    trans = trans / row_sums\n",
    "\n",
    "    return trans.flatten()\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Single-document Feature Extraction\n",
    "###############################################\n",
    "\n",
    "def extract_features_single(\n",
    "    text,\n",
    "    k,\n",
    "    tfidf_vectorizer,\n",
    "    alphabet,\n",
    "    fit_tfidf=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF + entropy + Markov vector.\n",
    "    All vectors are guaranteed fixed length if:\n",
    "    - TF-IDF was fit on the full training corpus\n",
    "    - alphabet is global\n",
    "    \"\"\"\n",
    "    pre = preprocess_text(text)\n",
    "    kmers = get_kmers(pre, k)\n",
    "\n",
    "    entropy = shannon_entropy(pre)\n",
    "    markov_vec = markov_features(pre, alphabet)\n",
    "\n",
    "    kmers_str = \" \".join(kmers)\n",
    "\n",
    "    if fit_tfidf:\n",
    "        tfidf_vec = tfidf_vectorizer.fit_transform([kmers_str]).toarray()[0]\n",
    "    else:\n",
    "        tfidf_vec = tfidf_vectorizer.transform([kmers_str]).toarray()[0]\n",
    "\n",
    "    return np.concatenate([tfidf_vec, [entropy], markov_vec])\n",
    "\n",
    "\n",
    "###############################################\n",
    "# TRAINING PIPELINE\n",
    "###############################################\n",
    "\n",
    "def train_pipeline_whole_docs(texts, labels, k=3):\n",
    "    \"\"\"\n",
    "    Train using full documents (no windowing).\n",
    "    Ensures fixed feature vector sizes for all samples.\n",
    "    \"\"\"\n",
    "    processed_docs = [preprocess_text(t) for t in texts]\n",
    "\n",
    "    # Build global alphabet across all training docs\n",
    "    global_alphabet = sorted(list(set(\"\".join(processed_docs))))\n",
    "\n",
    "    # Build kmers for TF-IDF fitting\n",
    "    kmers_docs = [\" \".join(get_kmers(doc, k)) for doc in processed_docs]\n",
    "\n",
    "    # Fit a single TF-IDF over ALL training documents\n",
    "    tfidf_vec = TfidfVectorizer(analyzer=\"word\")\n",
    "    tfidf_vec.fit(kmers_docs)\n",
    "\n",
    "    # Extract features\n",
    "    feature_list = []\n",
    "    for doc in processed_docs:\n",
    "        f = extract_features_single(\n",
    "            doc,\n",
    "            k,\n",
    "            tfidf_vec,\n",
    "            global_alphabet,\n",
    "            fit_tfidf=False\n",
    "        )\n",
    "        feature_list.append(f)\n",
    "\n",
    "    X = np.vstack(feature_list)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=1000\n",
    "    )\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    return clf, scaler, tfidf_vec, global_alphabet\n",
    "\n",
    "\n",
    "###############################################\n",
    "# TESTING – WITH WINDOWING IF INPUT TOO BIG\n",
    "###############################################\n",
    "\n",
    "def classify_document(\n",
    "    text,\n",
    "    clf,\n",
    "    scaler,\n",
    "    tfidf_vec,\n",
    "    alphabet,\n",
    "    k=3,\n",
    "    max_window_chars=20000\n",
    "):\n",
    "    \"\"\"\n",
    "    If document is short -> classify whole.\n",
    "    If too long -> split into windows.\n",
    "    If ANY window predicts 1 -> output 1 (DNA found).\n",
    "    \"\"\"\n",
    "    if len(text) <= max_window_chars:\n",
    "        # No windowing\n",
    "        feats = extract_features_single(\n",
    "            text, k, tfidf_vec, alphabet, fit_tfidf=False\n",
    "        )\n",
    "        feats = scaler.transform([feats])\n",
    "        pred = clf.predict(feats)[0]\n",
    "        return pred\n",
    "\n",
    "    # WINDOWING\n",
    "    windows = [\n",
    "        text[i:i + max_window_chars]\n",
    "        for i in range(0, len(text), max_window_chars)\n",
    "    ]\n",
    "\n",
    "    for w in windows:\n",
    "        feats = extract_features_single(\n",
    "            w, k, tfidf_vec, alphabet, fit_tfidf=False\n",
    "        )\n",
    "        feats = scaler.transform([feats])\n",
    "        pred = clf.predict(feats)[0]\n",
    "        if pred == 1:\n",
    "            return 1\n",
    "\n",
    "    return 0  # none of the windows detected DNA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2ab6291-86f7-49aa-857c-826525a52501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6545 935 1320\n",
      "Start training...\n",
      "Training done.\n",
      "Predictions stored.\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "clf, scaler, tfidf_vec, alphabet = train_pipeline_whole_docs(X_train, y_train, k=3)\n",
    "\n",
    "print(\"Training done.\")\n",
    "\n",
    "preds=[]\n",
    "\n",
    "for test_doc in list(X_test):\n",
    "    pred = classify_document(\n",
    "    test_doc,\n",
    "    clf,\n",
    "    scaler,\n",
    "    tfidf_vec,\n",
    "    alphabet,\n",
    "    k=3)\n",
    "    preds.append(pred)\n",
    "\n",
    "preds = np.array(preds, dtype=np.int64)\n",
    "print(\"Predictions stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a16e173-5d2b-49ed-b72c-70855bcb2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# ### Test with single string\n",
    "# text = \t\"\"\"\n",
    "# catggggagctgggtaccgaggtgacaagacgcagggccgagattggtctgccgcgcactagggtacccgactcctattgctaactactctaagtttaggacccacgggaaactccaccaaggtcgctacatcagaggctaccaattatagtctccgatcgtcgaattctcataactcgtcaaaaacagttagctacccaatcagtaccgctagcttctatacaagacacaaattcaagatctgtattcggacaggttccaagtacccttgaagtacggaatctctagcgaacccaatttgtacgccgccaacaagtacttggtgcacagtaaaggcgagcccaattccgctaggccgacaattggccccggcctcacggcgacgtacctctgggcaaatctcatggcttctcgagatgcgtcccgggaattaagtcggtttaattactgacactgctgcttaccataagtaaacactatcacttaaatgaacgttaacg\n",
    "# \"\"\"\n",
    "# pred = classify_document(\n",
    "#     text,\n",
    "#     clf,\n",
    "#     scaler,\n",
    "#     tfidf_vec,\n",
    "#     alphabet,\n",
    "#     k=3)\n",
    "# print(\"Prediction:\",pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76973cdd-4c2d-4722-9f59-2fb105c6fc3e",
   "metadata": {},
   "source": [
    "### Test performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "177271db-cb22-4108-8ac5-a60c0a75b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9651515151515152\n",
      "Precision: 0.9904153354632588\n",
      "Recall   : 0.9393939393939394\n",
      "F1 Score : 0.9642301710730948\n"
     ]
    }
   ],
   "source": [
    "# With updated dataset (more adversarial cases)\n",
    "\n",
    "y_pred = preds\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "411044fa-c85c-4927-af5d-bce47017d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9878971255673222\n",
      "Precision: 0.9908536585365854\n",
      "Recall   : 0.9848484848484849\n",
      "F1 Score : 0.9878419452887538\n"
     ]
    }
   ],
   "source": [
    "# With earlier dataset\n",
    "y_pred = preds\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfc1c2-8294-4597-bf2c-4eb4a683e755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training and Predictions - 2. 1-dimensional CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "22124d89-1b78-4ee9-98f2-699501238fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "# 1️⃣ Preprocessing\n",
    "#######################################\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Keep all characters (since DNA may be obfuscated),\n",
    "    only remove whitespace.\n",
    "    \"\"\"\n",
    "    return text.lower().replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\")\n",
    "\n",
    "#######################################\n",
    "# 2️⃣ Shannon entropy\n",
    "#######################################\n",
    "def shannon_entropy(text):\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "    from math import log2\n",
    "    freq = {}\n",
    "    for c in text:\n",
    "        freq[c] = freq.get(c, 0) + 1\n",
    "    entropy = 0\n",
    "    for c in freq:\n",
    "        p = freq[c] / len(text)\n",
    "        entropy -= p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "#######################################\n",
    "# 3️⃣ Character Encoder (no LabelEncoder)\n",
    "#######################################\n",
    "class CharEncoder:\n",
    "    \"\"\"\n",
    "    Maps characters to integer IDs.\n",
    "    0 is reserved for padding.\n",
    "    Unknown characters map to 0 at test time.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char2id = {}\n",
    "        self.id2char = {}\n",
    "        self.fitted = False\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, texts):\n",
    "        chars = sorted(list(set(\"\".join(texts))))\n",
    "        # index 0 = padding\n",
    "        self.char2id = {c:i+1 for i,c in enumerate(chars)}\n",
    "        self.id2char = {i+1:c for i,c in enumerate(chars)}\n",
    "        self.vocab_size = len(self.char2id) + 1  # +1 for padding\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, text):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"CharEncoder not fitted.\")\n",
    "        # unknown characters mapped to 0\n",
    "        return np.array([self.char2id.get(c, 0) for c in text], dtype=np.int64)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts)\n",
    "        return [self.transform(t) for t in texts]\n",
    "\n",
    "#######################################\n",
    "# 4️⃣ CNN model\n",
    "#######################################\n",
    "class DNASequenceCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, num_classes=2,\n",
    "                 kernel_sizes=[3,5,7], num_filters=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc_entropy = nn.Linear(1, 16)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*num_filters + 16, num_classes)\n",
    "\n",
    "    def forward(self, x, entropy):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len] integer-encoded text\n",
    "        entropy: [batch, 1] global numeric feature\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)           # [B, L, E]\n",
    "        emb = emb.transpose(1,2)          # [B, E, L]\n",
    "\n",
    "        conv_outs = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pooled = [F.adaptive_max_pool1d(c,1).squeeze(-1) for c in conv_outs]\n",
    "\n",
    "        cnn_feat = torch.cat(pooled, dim=1)\n",
    "        entropy_feat = F.relu(self.fc_entropy(entropy))\n",
    "        feat = torch.cat([cnn_feat, entropy_feat], dim=1)\n",
    "\n",
    "        return self.fc(feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f6b67-729e-4990-9831-bb1cb8fc51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents (DNA-like + normal)\n",
    "texts = X_train\n",
    "labels = y_train\n",
    "\n",
    "# Preprocess\n",
    "texts_proc = [preprocess_text(t) for t in texts]\n",
    "\n",
    "# Encode characters\n",
    "char_encoder = CharEncoder()\n",
    "sequences = char_encoder.fit_transform(texts_proc)\n",
    "\n",
    "# Pad sequences manually\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "sequences_pad = [\n",
    "    np.pad(seq, (0, max_len - len(seq)), constant_values=0)\n",
    "    for seq in sequences\n",
    "]\n",
    "\n",
    "X = torch.tensor(sequences_pad, dtype=torch.long).to(torch.device(\"cpu\"))\n",
    "y = torch.tensor(labels, dtype=torch.long).to(torch.device(\"cpu\"))\n",
    "\n",
    "# Compute entropy feature\n",
    "entropies = torch.tensor(\n",
    "    [[shannon_entropy(t)] for t in texts_proc],\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DNASequenceCNN(\n",
    "    vocab_size=char_encoder.vocab_size,\n",
    "    embed_dim=32,\n",
    "    num_classes=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, entropies)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "#################################\n",
    "# Prediction on new document\n",
    "#################################\n",
    "\n",
    "preds_cnn = []\n",
    "probs_cnn = []\n",
    "for test_doc in list(X_test):\n",
    "    with torch.no_grad():\n",
    "        test_doc = preprocess_text(\n",
    "            \"YYZZXWWXYZXYZ some normal text\"\n",
    "        )\n",
    "    \n",
    "        # transform + pad safely\n",
    "        test_seq = char_encoder.transform(test_doc)\n",
    "        test_seq = np.pad(\n",
    "            test_seq,\n",
    "            (0, max(0, max_len - len(test_seq))),\n",
    "            constant_values=0\n",
    "        )\n",
    "        test_seq = torch.tensor([test_seq], dtype=torch.long)\n",
    "    \n",
    "        test_entropy = torch.tensor(\n",
    "            [[shannon_entropy(test_doc)]],\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        pred = model(test_seq, test_entropy)\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "    preds_cnn.append(torch.argmax(pred, dim=1).item())\n",
    "    probs_cnn.append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce4768-e2a1-46fd-b920-195ff79378a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dfa40-b6c7-4472-8da0-a6403e62420d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fdddb-3425-4852-ae16-50bb1a80c33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ca66-4da2-409c-800d-1fc7778eafee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d706ea-4462-4e1e-b87c-6d1a14f4a053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d483036-5b76-4830-ab40-1cfaaa062041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:apart_hack] *",
   "language": "python",
   "name": "conda-env-apart_hack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
