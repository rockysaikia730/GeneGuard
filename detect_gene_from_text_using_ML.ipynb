{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b1da43-ed69-4322-9eb8-4449d5405732",
   "metadata": {},
   "source": [
    "# Classify a text document as True/False based on whether it contains gene sequence or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c7e4a-abc3-42df-8ecd-25efae30af53",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae0f78a-c45b-497a-9ecc-9b9fe322c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, Tuple, Optional, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e3ab2d-6ce4-439e-b21e-aa9147c08d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aba70bb-b358-45bc-9549-3ee09b3e602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a0b46c-71b7-4e4d-8197-cc93e52760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223f5f00-f0a6-4d58-b91b-4315d14d9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0586bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    EasyOcrOptions,\n",
    "    AcceleratorOptions, \n",
    "    AcceleratorDevice\n",
    "    )\n",
    "from docling.document_converter import (ImageFormatOption, PdfFormatOption, DocumentConverter)\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa07f32-b735-48d4-afdc-758078fc99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c932766c-eabd-489f-a57a-c17046d0105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f167f6ca-e8bc-47e0-99f0-e5b3f67d9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4503b798-3cb2-44b9-b25c-3023fd5a2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a27f1dde-6548-4bba-b42b-326a43e1335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb796368-c972-44ce-98cf-a591cc3b2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd96ba-f09e-4ccc-ac2c-6170227c8698",
   "metadata": {},
   "source": [
    "## Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f79acc8a-3fdd-4d8a-9075-1bf86aee499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df):\n",
    "    # Compute split index - train and test\n",
    "    split_idx = int(len(df) * 0.80)  # first 80%\n",
    "    \n",
    "    # First 80% and remaining 20%\n",
    "    train = df[:split_idx]\n",
    "    test = df[split_idx:]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277090ee-c85b-40b1-ad81-60fc67be1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_lists(list_of_lists):\n",
    "    out = []\n",
    "    for lst in list_of_lists:\n",
    "        lst_copy = lst[:]            # avoid modifying original\n",
    "        random.shuffle(lst_copy)     # shuffle in place\n",
    "        out.append(lst_copy)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f23b499-bb4c-4fcb-b642-301b5557a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random strings of text\n",
    "\n",
    "CHARS = string.ascii_letters + string.digits + \" .,!?$%&*#@/\\n\"\n",
    "\n",
    "def random_garbage(n):\n",
    "    return ''.join(random.choices(CHARS, k=n))\n",
    "\n",
    "def generate_fake_content():\n",
    "    prefix_len = random.randint(0, 5000)\n",
    "    suffix_len = random.randint(0, 5000)\n",
    "\n",
    "    prefix = random_garbage(prefix_len)\n",
    "    suffix = random_garbage(suffix_len)\n",
    "\n",
    "    return f\"{prefix}{suffix}\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "4553511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        logging.getLogger(\"docling\").setLevel(logging.ERROR)\n",
    "        ocr_options = EasyOcrOptions()\n",
    "        ocr_options.lang = [\"en\"]\n",
    "        ocr_options.use_gpu = True\n",
    "        accel_opts = AcceleratorOptions(num_threads=4, device=AcceleratorDevice.AUTO)\n",
    "\n",
    "        ocr_pipeline_config = PdfPipelineOptions(\n",
    "                do_ocr=True,\n",
    "                ocr_options=ocr_options,\n",
    "                verbose=False,\n",
    "                accelerator_options=accel_opts\n",
    "        )\n",
    "        docling_format_options = {\n",
    "            InputFormat.IMAGE: ImageFormatOption(\n",
    "                pipeline_options=ocr_pipeline_config\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.converter  = DocumentConverter(\n",
    "            format_options=docling_format_options\n",
    "        )\n",
    "\n",
    "    def apply_ocr(self, source):\n",
    "        result = self.converter.convert(source).document\n",
    "        return \"\\n\".join([item.text for item in result.texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> Stashed changes
   "id": "61abad6e-6eae-4c7d-8c50-43e79ca9c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of common bioinformatics/genomic file extensions.\n",
    "# To identify files matching these extensions are classified as 'genomic data'.\n",
    "GENOMIC_EXTENSION = [\n",
    "    '.fastq', '.fq', '.fasta', '.fa', '.fna', '.gb', '.gff', '.gff3', '.gtf',\n",
    "    '.sam', '.bam', '.cram', '.vcf', '.bcf', '.wig', '.bed', '.bigwig', '.tbi',\n",
    "    '.tabix', '.h5', '.hdf5', # HDF5 often used for single-cell data (e.g., Anndata)\n",
    "    ]\n",
    "\n",
    "# A set of common document and image extensions for text extraction.\n",
    "DOCUMENT_EXTENSIONS = ['.txt', '.pdf', '.doc', '.docx']\n",
    "IMG_EXTENSION = [\".png\", \".jpg\", \".jpeg\"]\n",
    "\n",
    "OCRProcessor = DocumentProcessor()\n",
    "\n",
    "def analyze_file(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a file path to determine if it is genomic data or a document,\n",
    "    and returns the classification or the document content.\n",
    "\n",
    "    Args:\n",
    "        file_path: The full path to the file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (is_genomic_extension, file_extension, content_or_none_or_emptymessage).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return (False, None, None)\n",
    "\n",
    "    # Check for empty file before reading\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        return (False, None, \"EMPTY FILE\")\n",
    "\n",
    "    # Get the file name and extension\n",
    "    file_name = os.path.basename(file_path)\n",
    "    _, ext = os.path.splitext(file_name)\n",
    "    ext = ext.lower()\n",
    "    \n",
    "\n",
    "\n",
    "    # 1. Check for Genomic Data\n",
    "    if ext in GENOMIC_EXTENSIONS:\n",
    "        return (True, ext, None)\n",
    "\n",
    "    # 2. Check for Document Data and Extract Content\n",
    "    elif ext in DOCUMENT_EXTENSIONS:\n",
    "        if ext == '.txt':\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "                return (False, ext, content.strip())\n",
    "            except Exception as e:\n",
    "                return (False, ext, f\"Could not read text file: {e}\")\n",
    "\n",
    "        elif ext == '.pdf':\n",
    "            reader = PdfReader(file_path)\n",
    "            content = \"\".join([page.extract_text() for page in reader.pages])\n",
    "            if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "            return (False, ext, content)\n",
    "\n",
    "        elif ext in ['.doc', '.docx']:\n",
    "            document = Document(file_path)\n",
    "            content = \"\\n\".join([paragraph.text for paragraph in document.paragraphs])\n",
    "            if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "            return (False, ext, content)\n",
    "        \n",
    "        elif ext in IMG_EXTENSION:\n",
    "             content = OCRProcessor.apply_ocr(file_path)\n",
    "             if not content.strip():\n",
    "                    return (False, ext, \"Empty or non-text content\")\n",
    "             return (False, ext, content)\n",
    "\n",
    "    # 3. Handle Unknown/Other Files\n",
    "    return (False, ext, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d670ab71-c61c-47a5-a29c-f4e47810a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_genomic, file_type, content = analyze_file(\"xyz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91049169-68c0-4500-abea-1b5da7edd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_attempt_type = {1:\"Sequence (continuous) with English Text\", \n",
    "                            2:\"Sequence (continuous) with Random Text (All positive samples)\", \n",
    "                            3:\"Sequence with alternate random characters, bounded by English Text\",\n",
    "                            4:\"Sequence compressed and obfuscated with English Text\",\n",
    "                            5:\"Sequence compressed and replaced by other characters\",\n",
    "                            6:\"Sequence replaced by other characters\",\n",
    "                            7:\"Fake DNA sequences (All negative samples)\",\n",
    "                            8:\"Sequence replaced with multi-character mapping(length:1-5) and bounded with English Text\",\n",
    "                            9:\"Sequence broken with random spaces, bounded by English Text\",\n",
    "                            10:\"Sequence broken by random strings of text (variable length)\",\n",
    "                            11:\"Randomly generated characters (All negative samples)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9873f-95e6-4562-8994-dad19e1a781b",
   "metadata": {},
   "source": [
    "## Load data, prepare train and test data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "728cf29c-b494-43ad-bb44-a5337fff0312",
   "metadata": {},
   "source": [
    "### Curating our dataset -> Methodology\n",
    "\n",
    "To train our ML model:\n",
    "\n",
    "1) english and seq -> pos(actual), pos(actual+english), neg(english)\n",
    "                   200          200                  200          \n",
    "\n",
    "2) text and seq -> pos(actual), pos(actual+text) \n",
    "                1000         1000               \n",
    "\n",
    "3) text_and_seq_alternate_random_characters -> pos(actual), pos(actual with alt rand char+english), neg(english)\n",
    "                                            200          200                                     200            \n",
    "\n",
    "4) text and seq compressed -> pos(actual), pos(actual compressed), pos(actual compressed + english), neg(english)\n",
    "                           200          200                     200                               200           \n",
    "\n",
    "5) text and seq compressed and replaced -> pos (actual), pos (actual compressed), pos (compressed_replaced + english), neg (english)\n",
    "                                        200           200                      200                                  200              \n",
    "\n",
    "6) text and seq replaced with other characters -> pos(actual), pos(replaced + english), neg(english)\n",
    "                                            200             200                      200            \n",
    "\n",
    "7) single (fake) dna -> neg\n",
    "                    100  \n",
    "\n",
    "\n",
    "8) multicharacter mapping -> pos (actual), pos (multicharacter + english), neg (english)\n",
    "                          200           200                             200                \n",
    "\n",
    "9) sequence with random space and english text -> pos(actual), pos(actual with random space), pos (actual with random space + english), neg (english)\n",
    "                                                200         200                            200                                       200\n",
    "\n",
    "\n",
    "10) sequence with with text breaks -> pos(actual), pos(actual with english text breaks), neg(english)\n",
    "                                  200          200                                   200\n",
    "\n",
    "11) randomly generated text -> neg\n",
    "                               4100  (calculated based on above imbalance of pos and neg)\n",
    "\n",
    "\n",
    "Total input data samples = 11600\n",
    "Total pos = 5800 (50%)\n",
    "Total neg = 5800 (50%)\n",
    "Train = 80%, Test = 20%\n",
    "\n",
    "\n",
    "Distributions are same across train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cec31fe-6b0e-4799-88f6-c91fdb4c1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train=[]\n",
    "neg_train=[]\n",
    "\n",
    "pos_test=[]\n",
    "pos_test_type=[] # adversarial attack type\n",
    "\n",
    "neg_test=[]\n",
    "neg_test_type=[] # adversarial attack type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07190642-1ae9-432e-b6f8-05b92b985a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb451c50-9b97-456a-976f-1ded2dd5cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"english_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [1]*len(p1_test + p2_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [1]*len(n1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "befda429-92ce-4a1f-b5f8-c551e622a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "\n",
    "path = \"text_and_seq\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['generated'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [2]*len(p1_test + p2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55f10519-578b-414f-b779-7ccb4ade3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"text_and_seq_alternate_random_characters\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [3]*len(p1_test + p2_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [3]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abb80e68-da1f-4d77-aa2e-38ca15a468a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "p3=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"text_and_seq_compressed\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['compressed'].values[0])\n",
    "    p3.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "p3_train, p3_test = split_train_test(p3)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train + p3_train\n",
    "pos_test = pos_test + p1_test + p2_test + p3_test\n",
    "pos_test_type = pos_test_type + [4]*len(p1_test + p2_test + p3_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [4]*len(n1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcb80c18-261f-4d2f-a3b0-52d8d167b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "p3=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"text_and_seq_compressed_and_replaced\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['compressed'].values[0])\n",
    "    p3.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "p3_train, p3_test = split_train_test(p3)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train + p3_train\n",
    "pos_test = pos_test + p1_test + p2_test + p3_test\n",
    "pos_test_type = pos_test_type + [5]*len(p1_test + p2_test + p3_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [5]*len(n1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dc62e55-9781-4d8d-9ac1-92bea9c248cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=[]\n",
    "p2=[]\n",
    "n1=[]\n",
    "\n",
    "path = \"text_and_seq_replaced_with_other_characters\"\n",
    "for file_name in os.listdir(path):\n",
    "    file_path = path + \"/\" + file_name\n",
    "    if (not (file_name.endswith(\".csv\") | file_name.endswith(\".txt\"))):\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    p1.append(df['sequence'].values[0])\n",
    "    p2.append(df['text_with_dna'].values[0])\n",
    "    n1.append(df['text_without_dna'].values[0])\n",
    "\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [6]*len(p1_test + p2_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [6]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a9aad32-cca5-4e90-af00-3ec42d5e6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = []\n",
    "for file in os.listdir('single_dna'):\n",
    "    try:\n",
    "        with open('single_dna/'+file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "    except:\n",
    "        continue\n",
    "    n1.append(text)\n",
    "n1 = [s for s in n1 if len(s) <= 1000]\n",
    "n1 = random.sample(n1, 100)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [7]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065cfc7-e4b2-4bf8-a003-cdcda3217833",
   "metadata": {},
   "outputs": [],
   "source": [
    "8) multicharacter mapping -> pos (actual), pos (multicharacter + english), neg (english)\n",
    "                          200           200                             200                \n",
    "\n",
    "9) sequence with random space and english text -> pos(actual), pos(actual with random space), pos (actual with random space + english), neg (english)\n",
    "                                                200         200                            200                                       200\n",
    "\n",
    "\n",
    "10) sequence with with text breaks -> pos(actual), pos(actual with english text breaks), neg(english)\n",
    "                                  200          200                                   200\n",
    "\n",
    "11) randomly generated text -> neg\n",
    "                               4100  (calculated based on above imbalance of pos and neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e390d880-9933-47d6-bfbe-1d968e408c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('multicharacter_mapping.pkl') #sequence, text_with_dna, text_without_dna\n",
    "p1 = list(df['sequence'].values)\n",
    "p2 = list(df['text_with_dna'].values)\n",
    "n1 = list(df['text_without_dna'].values)\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [8]*len(p1_test + p2_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [8]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bafdb80-7ed7-4b85-a167-20ac5b3ee89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sequence_with_random_space_and_english_text.pkl') #sequence, text_with_dna, text_without_dna\n",
    "p1 = list(df['sequence'].values)\n",
    "p2 = list(df['text_with_dna'].values)\n",
    "p3 = list(df['sequence_with_random_space'].values)\n",
    "n1 = list(df['text_without_dna'].values)\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "p3_train, p3_test = split_train_test(p3)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train + p3_train\n",
    "pos_test = pos_test + p1_test + p2_test + p3_test\n",
    "pos_test_type = pos_test_type + [9]*len(p1_test + p2_test + p3_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [9]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9701d51-29fc-497f-b7bf-82827d2bbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('sequence_with_text_breaks.pkl') #sequence, text_with_dna, text_without_dna\n",
    "p1 = list(df['sequence'].values)\n",
    "p2 = list(df['text_with_dna'].values)\n",
    "n1 = list(df['text_without_dna'].values)\n",
    "p1_train, p1_test = split_train_test(p1)\n",
    "p2_train, p2_test = split_train_test(p2)\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "# add to global lists\n",
    "pos_train = pos_train + p1_train + p2_train\n",
    "pos_test = pos_test + p1_test + p2_test\n",
    "pos_test_type = pos_test_type + [10]*len(p1_test + p2_test)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [10]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb39915d-525a-4495-92d9-bca06490fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [] \n",
    "\n",
    "for i in range(4100):\n",
    "    n1.append(generate_fake_content())\n",
    "\n",
    "n1_train, n1_test = split_train_test(n1)\n",
    "neg_train = neg_train + n1_train\n",
    "neg_test = neg_test + n1_test\n",
    "neg_test_type = neg_test_type + [11]*len(n1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6529c4ef-9d65-41c0-83c6-613c6673870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640, 1160, 1160, 4640, 1160, 1160)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train), len(pos_test), len(pos_test_type), len(neg_train), len(neg_test), len(neg_test_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07201c-cfd7-471e-b6f3-7a74a9cb759e",
   "metadata": {},
   "source": [
    "### Build train and test set from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04118e7d-1d7e-4244-b28b-96f5f6581e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640, 1160, 4640, 1160)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train), len(pos_test), len(neg_train), len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "735cdbd2-e6f9-4f54-8b54-cc73a9877b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pos_train\n",
    "test_pos = pos_test\n",
    "train_neg = neg_train\n",
    "test_neg = neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e9b97ae-40b9-4173-8c1e-422f7becc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "\n",
    "# create pos dataframe\n",
    "df_train_pos = pd.DataFrame(train_pos, columns=[\"data\"])\n",
    "df_train_pos[\"label\"] = 1\n",
    "\n",
    "# create neg dataframe\n",
    "df_train_neg = pd.DataFrame(train_neg, columns=[\"data\"])\n",
    "df_train_neg[\"label\"] = 0\n",
    "\n",
    "# combine\n",
    "df_combined = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "\n",
    "#shuffle\n",
    "df_shuffled = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_shuffled.to_pickle('df_train_final.pkl')\n",
    "\n",
    "X = df_shuffled[\"data\"].values\n",
    "y = df_shuffled[\"label\"].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,\n",
    "                     test_size=0.1,\n",
    "                     random_state=42,\n",
    "                     stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "babb9192-9bc8-47b9-90b0-87d41f7d4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET\n",
    "\n",
    "# create pos dataframe\n",
    "df_test_pos = pd.DataFrame({\"data\": test_pos, \"type\": pos_test_type})\n",
    "df_test_pos[\"label\"] = 1\n",
    "\n",
    "# create neg dataframe\n",
    "df_test_neg = pd.DataFrame({\"data\": test_neg, \"type\": neg_test_type})\n",
    "df_test_neg[\"label\"] = 0\n",
    "\n",
    "# combine\n",
    "df_combined = pd.concat([df_test_pos, df_test_neg], ignore_index=True)\n",
    "\n",
    "df_combined.to_pickle('df_test_final.pkl')\n",
    "\n",
    "X_test = df_combined[\"data\"].values\n",
    "y_test = df_combined[\"label\"].values\n",
    "type_test = df_combined[\"type\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f560f-0abc-4acc-a345-5095d9efd283",
   "metadata": {},
   "source": [
    "## Training and Predictions - 1. Preprocessing + ML (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4968fc2-9180-4152-9654-1651696268d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "# Preprocessing\n",
    "###############################################\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Very minimal preprocessing:\n",
    "    - Lowercase\n",
    "    - Remove whitespace newlines\n",
    "    We DO NOT filter characters, because DNA may be obfuscated.\n",
    "    \"\"\"\n",
    "    return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "###############################################\n",
    "# K-mer Extraction\n",
    "###############################################\n",
    "\n",
    "def get_kmers(text, k=3):\n",
    "    \"\"\"Extract k-mers without filtering characters.\"\"\"\n",
    "    kmers = []\n",
    "    for i in range(len(text) - k + 1):\n",
    "        kmers.append(text[i:i+k])\n",
    "    return kmers\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Entropy Calculation\n",
    "###############################################\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy over all characters.\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    from math import log2\n",
    "    freq = {}\n",
    "    for c in text:\n",
    "        freq[c] = freq.get(c, 0) + 1\n",
    "\n",
    "    entropy = 0\n",
    "    for c in freq:\n",
    "        p = freq[c] / len(text)\n",
    "        entropy -= p * log2(p)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Markov Transition Features\n",
    "###############################################\n",
    "\n",
    "def markov_features(text, alphabet):\n",
    "    \"\"\"\n",
    "    Build a Markov transition probability vector from:\n",
    "    P(next_char | current_char)\n",
    "\n",
    "    The output is flattened row-major into a single vector.\n",
    "    \"\"\"\n",
    "    if len(text) < 2:\n",
    "        return np.zeros(len(alphabet) * len(alphabet))\n",
    "\n",
    "    index = {c: i for i, c in enumerate(alphabet)}\n",
    "\n",
    "    # Transition counts\n",
    "    trans = np.zeros((len(alphabet), len(alphabet)))\n",
    "\n",
    "    for a, b in zip(text[:-1], text[1:]):\n",
    "        if a in index and b in index:\n",
    "            trans[index[a], index[b]] += 1\n",
    "\n",
    "    row_sums = trans.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  \n",
    "    trans = trans / row_sums\n",
    "\n",
    "    return trans.flatten()\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Single-document Feature Extraction\n",
    "###############################################\n",
    "\n",
    "def extract_features_single(\n",
    "    text,\n",
    "    k,\n",
    "    tfidf_vectorizer,\n",
    "    alphabet,\n",
    "    fit_tfidf=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF + entropy + Markov vector.\n",
    "    All vectors are guaranteed fixed length if:\n",
    "    - TF-IDF was fit on the full training corpus\n",
    "    - alphabet is global\n",
    "    \"\"\"\n",
    "    pre = preprocess_text(text)\n",
    "    kmers = get_kmers(pre, k)\n",
    "\n",
    "    entropy = shannon_entropy(pre)\n",
    "    markov_vec = markov_features(pre, alphabet)\n",
    "\n",
    "    kmers_str = \" \".join(kmers)\n",
    "\n",
    "    if fit_tfidf:\n",
    "        tfidf_vec = tfidf_vectorizer.fit_transform([kmers_str]).toarray()[0]\n",
    "    else:\n",
    "        tfidf_vec = tfidf_vectorizer.transform([kmers_str]).toarray()[0]\n",
    "\n",
    "    return np.concatenate([tfidf_vec, [entropy], markov_vec])\n",
    "\n",
    "\n",
    "###############################################\n",
    "# TRAINING PIPELINE\n",
    "###############################################\n",
    "\n",
    "def train_pipeline_whole_docs(texts, labels, k=3):\n",
    "    \"\"\"\n",
    "    Train using full documents (no windowing).\n",
    "    Ensures fixed feature vector sizes for all samples.\n",
    "    \"\"\"\n",
    "    processed_docs = [preprocess_text(t) for t in texts]\n",
    "\n",
    "    # Build global alphabet across all training docs\n",
    "    global_alphabet = sorted(list(set(\"\".join(processed_docs))))\n",
    "\n",
    "    # Build kmers for TF-IDF fitting\n",
    "    kmers_docs = [\" \".join(get_kmers(doc, k)) for doc in processed_docs]\n",
    "\n",
    "    # Fit a single TF-IDF over ALL training documents\n",
    "    tfidf_vec = TfidfVectorizer(analyzer=\"word\")\n",
    "    tfidf_vec.fit(kmers_docs)\n",
    "\n",
    "    # Extract features\n",
    "    feature_list = []\n",
    "    for doc in processed_docs:\n",
    "        f = extract_features_single(\n",
    "            doc,\n",
    "            k,\n",
    "            tfidf_vec,\n",
    "            global_alphabet,\n",
    "            fit_tfidf=False\n",
    "        )\n",
    "        feature_list.append(f)\n",
    "\n",
    "    X = np.vstack(feature_list)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=1000\n",
    "    )\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    return clf, scaler, tfidf_vec, global_alphabet\n",
    "\n",
    "\n",
    "###############################################\n",
    "# TESTING â€“ WITH WINDOWING IF INPUT TOO BIG\n",
    "###############################################\n",
    "\n",
    "def classify_document(\n",
    "    text,\n",
    "    clf,\n",
    "    scaler,\n",
    "    tfidf_vec,\n",
    "    alphabet,\n",
    "    k=3,\n",
    "    max_window_chars=20000\n",
    "):\n",
    "    \"\"\"\n",
    "    If document is short -> classify whole.\n",
    "    If too long -> split into windows.\n",
    "    If ANY window predicts 1 -> output 1 (DNA found).\n",
    "    \"\"\"\n",
    "    if len(text) <= max_window_chars:\n",
    "        # No windowing\n",
    "        feats = extract_features_single(\n",
    "            text, k, tfidf_vec, alphabet, fit_tfidf=False\n",
    "        )\n",
    "        feats = scaler.transform([feats])\n",
    "        pred = clf.predict(feats)[0]\n",
    "        return pred\n",
    "\n",
    "    # WINDOWING\n",
    "    windows = [\n",
    "        text[i:i + max_window_chars]\n",
    "        for i in range(0, len(text), max_window_chars)\n",
    "    ]\n",
    "\n",
    "    for w in windows:\n",
    "        feats = extract_features_single(\n",
    "            w, k, tfidf_vec, alphabet, fit_tfidf=False\n",
    "        )\n",
    "        feats = scaler.transform([feats])\n",
    "        pred = clf.predict(feats)[0]\n",
    "        if pred == 1:\n",
    "            return 1\n",
    "\n",
    "    return 0  # none of the windows detected DNA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f2ab6291-86f7-49aa-857c-826525a52501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8352 928 2320\n",
      "Start training...\n",
      "Training done.\n",
      "Predictions stored.\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "clf, scaler, tfidf_vec, alphabet = train_pipeline_whole_docs(X_train, y_train, k=3)\n",
    "\n",
    "print(\"Training done.\")\n",
    "\n",
    "preds=[]\n",
    "\n",
    "for test_doc in list(X_test):\n",
    "    pred = classify_document(\n",
    "    test_doc,\n",
    "    clf,\n",
    "    scaler,\n",
    "    tfidf_vec,\n",
    "    alphabet,\n",
    "    k=3)\n",
    "    preds.append(pred)\n",
    "\n",
    "preds = np.array(preds, dtype=np.int64)\n",
    "print(\"Predictions stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a16e173-5d2b-49ed-b72c-70855bcb2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# ### Test with single string\n",
    "# text = \t\"\"\"\n",
    "# catggggagctgggtaccgaggtgacaagacgcagggccgagattggtctgccgcgcactagggtacccgactcctattgctaactactctaagtttaggacccacgggaaactccaccaaggtcgctacatcagaggctaccaattatagtctccgatcgtcgaattctcataactcgtcaaaaacagttagctacccaatcagtaccgctagcttctatacaagacacaaattcaagatctgtattcggacaggttccaagtacccttgaagtacggaatctctagcgaacccaatttgtacgccgccaacaagtacttggtgcacagtaaaggcgagcccaattccgctaggccgacaattggccccggcctcacggcgacgtacctctgggcaaatctcatggcttctcgagatgcgtcccgggaattaagtcggtttaattactgacactgctgcttaccataagtaaacactatcacttaaatgaacgttaacg\n",
    "# \"\"\"\n",
    "# pred = classify_document(\n",
    "#     text,\n",
    "#     clf,\n",
    "#     scaler,\n",
    "#     tfidf_vec,\n",
    "#     alphabet,\n",
    "#     k=3)\n",
    "# print(\"Prediction:\",pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76973cdd-4c2d-4722-9f59-2fb105c6fc3e",
   "metadata": {},
   "source": [
    "### Test performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "809372d0-a829-4efe-876f-fde4e31baddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9629310344827586\n",
      "Precision: 0.9890710382513661\n",
      "Recall   : 0.9362068965517242\n",
      "F1 Score : 0.9619131975199291\n"
     ]
    }
   ],
   "source": [
    "# With updated dataset (final - all adversarial cases)\n",
    "\n",
    "y_pred = preds\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ab96d0f-6f4d-458e-8666-3c9309bc0cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Attempt Type: Sequence (continuous) with English Text\n",
      "Accuracy : 0.9833333333333333\n",
      "Precision: 1.0\n",
      "Recall   : 0.975\n",
      "F1 Score : 0.9873417721518988\n",
      "Adversarial Attempt Type: Sequence (continuous) with Random Text (All positive samples)\n",
      "Accuracy : 0.9425\n",
      "Precision: 1.0\n",
      "Recall   : 0.9425\n",
      "F1 Score : 0.9703989703989704\n",
      "Adversarial Attempt Type: Sequence with alternate random characters, bounded by English Text\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence compressed and obfuscated with English Text\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence compressed and replaced by other characters\n",
      "Accuracy : 0.84375\n",
      "Precision: 1.0\n",
      "Recall   : 0.7916666666666666\n",
      "F1 Score : 0.8837209302325582\n",
      "Adversarial Attempt Type: Sequence replaced by other characters\n",
      "Accuracy : 0.9166666666666666\n",
      "Precision: 1.0\n",
      "Recall   : 0.875\n",
      "F1 Score : 0.9333333333333333\n",
      "Adversarial Attempt Type: Fake DNA sequences (All negative samples)\n",
      "Accuracy : 0.6\n",
      "Precision: 0.0\n",
      "Recall   : 0.0\n",
      "F1 Score : 0.0\n",
      "Adversarial Attempt Type: Sequence replaced with multi-character mapping(length:1-5) and bounded with English Text\n",
      "Accuracy : 0.925\n",
      "Precision: 1.0\n",
      "Recall   : 0.8875\n",
      "F1 Score : 0.9403973509933775\n",
      "Adversarial Attempt Type: Sequence broken with random spaces, bounded by English Text\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence broken by random strings of text (variable length)\n",
      "Accuracy : 0.9583333333333334\n",
      "Precision: 1.0\n",
      "Recall   : 0.9375\n",
      "F1 Score : 0.967741935483871\n",
      "Adversarial Attempt Type: Randomly generated characters (All negative samples)\n",
      "Accuracy : 0.9951219512195122\n",
      "Precision: 0.0\n",
      "Recall   : 0.0\n",
      "F1 Score : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in list(set(type_test)):\n",
    "    print(\"Adversarial Attempt Type:\", adversarial_attempt_type[i])\n",
    "    y_pred_temp = y_pred[type_test==i]\n",
    "    y_test_temp = y_test[type_test==i]\n",
    "    # if (len(set(y_test_temp))<2):\n",
    "    #     continue\n",
    "    acc  = accuracy_score(y_test_temp, y_pred_temp)\n",
    "    prec = precision_score(y_test_temp, y_pred_temp)\n",
    "    rec  = recall_score(y_test_temp, y_pred_temp)\n",
    "    f1   = f1_score(y_test_temp, y_pred_temp)\n",
    "    print(\"Accuracy :\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall   :\", rec)\n",
    "    print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "177271db-cb22-4108-8ac5-a60c0a75b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9651515151515152\n",
      "Precision: 0.9904153354632588\n",
      "Recall   : 0.9393939393939394\n",
      "F1 Score : 0.9642301710730948\n"
     ]
    }
   ],
   "source": [
    "# With updated dataset (more adversarial cases) (test_new, train_new)\n",
    "\n",
    "y_pred = preds\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "411044fa-c85c-4927-af5d-bce47017d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9629310344827586\n",
      "Precision: 0.9890710382513661\n",
      "Recall   : 0.9362068965517242\n",
      "F1 Score : 0.9619131975199291\n"
     ]
    }
   ],
   "source": [
    "# With final dataset (test_final, train_final)\n",
    "y_pred = preds\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd59cc1-e917-4308-896c-7ebb73e371e2",
   "metadata": {},
   "source": [
    "## Use simple heuristics for flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3619aebe-dbae-432f-b78e-7c84ee1cebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heuristics.simple_check import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41684cd4-80c7-49bb-bc05-189e0da831d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions stored.\n"
     ]
    }
   ],
   "source": [
    "preds_heuristics=[]\n",
    "\n",
    "for test_doc in list(X_test):\n",
    "    pred = check(test_doc)\n",
    "    preds_heuristics.append(pred)\n",
    "\n",
    "preds_heuristics = np.array(preds_heuristics, dtype=np.int64)\n",
    "print(\"Predictions stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67b570ee-8229-48f2-bb7f-56832af94a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8685344827586207\n",
      "Precision: 0.9776536312849162\n",
      "Recall   : 0.7543103448275862\n",
      "F1 Score : 0.851581508515815\n"
     ]
    }
   ],
   "source": [
    "# With final dataset\n",
    "\n",
    "y_pred = preds_heuristics\n",
    "acc  = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec  = recall_score(y_test, y_pred)\n",
    "f1   = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33d4cb1d-8b18-40d3-921a-97414bd7f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Attempt Type: Sequence (continuous) with English Text\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence (continuous) with Random Text (All positive samples)\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence with alternate random characters, bounded by English Text\n",
      "Accuracy : 0.6666666666666666\n",
      "Precision: 1.0\n",
      "Recall   : 0.5\n",
      "F1 Score : 0.6666666666666666\n",
      "Adversarial Attempt Type: Sequence compressed and obfuscated with English Text\n",
      "Accuracy : 0.50625\n",
      "Precision: 1.0\n",
      "Recall   : 0.3416666666666667\n",
      "F1 Score : 0.5093167701863354\n",
      "Adversarial Attempt Type: Sequence compressed and replaced by other characters\n",
      "Accuracy : 0.5125\n",
      "Precision: 1.0\n",
      "Recall   : 0.35\n",
      "F1 Score : 0.5185185185185185\n",
      "Adversarial Attempt Type: Sequence replaced by other characters\n",
      "Accuracy : 0.675\n",
      "Precision: 1.0\n",
      "Recall   : 0.5125\n",
      "F1 Score : 0.6776859504132231\n",
      "Adversarial Attempt Type: Fake DNA sequences (All negative samples)\n",
      "Accuracy : 0.0\n",
      "Precision: 0.0\n",
      "Recall   : 0.0\n",
      "F1 Score : 0.0\n",
      "Adversarial Attempt Type: Sequence replaced with multi-character mapping(length:1-5) and bounded with English Text\n",
      "Accuracy : 0.6666666666666666\n",
      "Precision: 1.0\n",
      "Recall   : 0.5\n",
      "F1 Score : 0.6666666666666666\n",
      "Adversarial Attempt Type: Sequence broken with random spaces, bounded by English Text\n",
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n",
      "Adversarial Attempt Type: Sequence broken by random strings of text (variable length)\n",
      "Accuracy : 0.925\n",
      "Precision: 1.0\n",
      "Recall   : 0.8875\n",
      "F1 Score : 0.9403973509933775\n",
      "Adversarial Attempt Type: Randomly generated characters (All negative samples)\n",
      "Accuracy : 1.0\n",
      "Precision: 0.0\n",
      "Recall   : 0.0\n",
      "F1 Score : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/apart_hack/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in list(set(type_test)):\n",
    "    print(\"Adversarial Attempt Type:\", adversarial_attempt_type[i])\n",
    "    y_pred_temp = y_pred[type_test==i]\n",
    "    y_test_temp = y_test[type_test==i]\n",
    "    # if (len(set(y_test_temp))<2):\n",
    "    #     continue\n",
    "    acc  = accuracy_score(y_test_temp, y_pred_temp)\n",
    "    prec = precision_score(y_test_temp, y_pred_temp)\n",
    "    rec  = recall_score(y_test_temp, y_pred_temp)\n",
    "    f1   = f1_score(y_test_temp, y_pred_temp)\n",
    "    print(\"Accuracy :\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall   :\", rec)\n",
    "    print(\"F1 Score :\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfc1c2-8294-4597-bf2c-4eb4a683e755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training and Predictions - 2. 1-dimensional CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "22124d89-1b78-4ee9-98f2-699501238fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "# 1ï¸âƒ£ Preprocessing\n",
    "#######################################\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Keep all characters (since DNA may be obfuscated),\n",
    "    only remove whitespace.\n",
    "    \"\"\"\n",
    "    return text.lower().replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\")\n",
    "\n",
    "#######################################\n",
    "# 2ï¸âƒ£ Shannon entropy\n",
    "#######################################\n",
    "def shannon_entropy(text):\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "    from math import log2\n",
    "    freq = {}\n",
    "    for c in text:\n",
    "        freq[c] = freq.get(c, 0) + 1\n",
    "    entropy = 0\n",
    "    for c in freq:\n",
    "        p = freq[c] / len(text)\n",
    "        entropy -= p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "#######################################\n",
    "# 3ï¸âƒ£ Character Encoder (no LabelEncoder)\n",
    "#######################################\n",
    "class CharEncoder:\n",
    "    \"\"\"\n",
    "    Maps characters to integer IDs.\n",
    "    0 is reserved for padding.\n",
    "    Unknown characters map to 0 at test time.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char2id = {}\n",
    "        self.id2char = {}\n",
    "        self.fitted = False\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, texts):\n",
    "        chars = sorted(list(set(\"\".join(texts))))\n",
    "        # index 0 = padding\n",
    "        self.char2id = {c:i+1 for i,c in enumerate(chars)}\n",
    "        self.id2char = {i+1:c for i,c in enumerate(chars)}\n",
    "        self.vocab_size = len(self.char2id) + 1  # +1 for padding\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, text):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"CharEncoder not fitted.\")\n",
    "        # unknown characters mapped to 0\n",
    "        return np.array([self.char2id.get(c, 0) for c in text], dtype=np.int64)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts)\n",
    "        return [self.transform(t) for t in texts]\n",
    "\n",
    "#######################################\n",
    "# 4ï¸âƒ£ CNN model\n",
    "#######################################\n",
    "class DNASequenceCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, num_classes=2,\n",
    "                 kernel_sizes=[3,5,7], num_filters=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc_entropy = nn.Linear(1, 16)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*num_filters + 16, num_classes)\n",
    "\n",
    "    def forward(self, x, entropy):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len] integer-encoded text\n",
    "        entropy: [batch, 1] global numeric feature\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)           # [B, L, E]\n",
    "        emb = emb.transpose(1,2)          # [B, E, L]\n",
    "\n",
    "        conv_outs = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pooled = [F.adaptive_max_pool1d(c,1).squeeze(-1) for c in conv_outs]\n",
    "\n",
    "        cnn_feat = torch.cat(pooled, dim=1)\n",
    "        entropy_feat = F.relu(self.fc_entropy(entropy))\n",
    "        feat = torch.cat([cnn_feat, entropy_feat], dim=1)\n",
    "\n",
    "        return self.fc(feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f6b67-729e-4990-9831-bb1cb8fc51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents (DNA-like + normal)\n",
    "texts = X_train\n",
    "labels = y_train\n",
    "\n",
    "# Preprocess\n",
    "texts_proc = [preprocess_text(t) for t in texts]\n",
    "\n",
    "# Encode characters\n",
    "char_encoder = CharEncoder()\n",
    "sequences = char_encoder.fit_transform(texts_proc)\n",
    "\n",
    "# Pad sequences manually\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "sequences_pad = [\n",
    "    np.pad(seq, (0, max_len - len(seq)), constant_values=0)\n",
    "    for seq in sequences\n",
    "]\n",
    "\n",
    "X = torch.tensor(sequences_pad, dtype=torch.long).to(torch.device(\"cpu\"))\n",
    "y = torch.tensor(labels, dtype=torch.long).to(torch.device(\"cpu\"))\n",
    "\n",
    "# Compute entropy feature\n",
    "entropies = torch.tensor(\n",
    "    [[shannon_entropy(t)] for t in texts_proc],\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = DNASequenceCNN(\n",
    "    vocab_size=char_encoder.vocab_size,\n",
    "    embed_dim=32,\n",
    "    num_classes=2\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X, entropies)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "#################################\n",
    "# Prediction on new document\n",
    "#################################\n",
    "\n",
    "preds_cnn = []\n",
    "probs_cnn = []\n",
    "for test_doc in list(X_test):\n",
    "    with torch.no_grad():\n",
    "        test_doc = preprocess_text(\n",
    "            \"YYZZXWWXYZXYZ some normal text\"\n",
    "        )\n",
    "    \n",
    "        # transform + pad safely\n",
    "        test_seq = char_encoder.transform(test_doc)\n",
    "        test_seq = np.pad(\n",
    "            test_seq,\n",
    "            (0, max(0, max_len - len(test_seq))),\n",
    "            constant_values=0\n",
    "        )\n",
    "        test_seq = torch.tensor([test_seq], dtype=torch.long)\n",
    "    \n",
    "        test_entropy = torch.tensor(\n",
    "            [[shannon_entropy(test_doc)]],\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        pred = model(test_seq, test_entropy)\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "    preds_cnn.append(torch.argmax(pred, dim=1).item())\n",
    "    probs_cnn.append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce4768-e2a1-46fd-b920-195ff79378a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dfa40-b6c7-4472-8da0-a6403e62420d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fdddb-3425-4852-ae16-50bb1a80c33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ca66-4da2-409c-800d-1fc7778eafee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d706ea-4462-4e1e-b87c-6d1a14f4a053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d483036-5b76-4830-ab40-1cfaaa062041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:apart_hack] *",
   "language": "python",
   "name": "conda-env-apart_hack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
